{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "794145a8-d1b7-41b8-8d8c-d3094f46a2b0",
   "metadata": {},
   "source": [
    "# MedVQA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90a825c-0830-43ea-bff5-60f9544df259",
   "metadata": {},
   "source": [
    "Outline\n",
    "\n",
    "- import data / create dataloader classes\n",
    "- import text and vision model / create multimodal medvqa model that inherits from hf\n",
    "- import hf trainer and create model trainer that inherits from hf\n",
    "- train on base model / data, then fine tune model from checkpoint using anything (radiology in practice, slate for demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5f5490-4bd9-44d8-b108-de88d3627231",
   "metadata": {},
   "source": [
    "Data:\n",
    "https://huggingface.co/datasets/xmcmic/PMC-VQA/tree/main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02162b70-a62f-47dd-b510-01cc7a63fc5e",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "- bert tokenization of text in dataset wrapper\n",
    "- data augmentation python script\n",
    "- peft + clip models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d52c7640-119b-4508-9c5a-054fc7383f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from dataclasses import dataclass, field\n",
    "import transformers\n",
    "from transformers import AutoModel,BertConfig,AutoTokenizer,AutoProcessor,LlamaTokenizer, CLIPVisionModel, CLIPVisionConfig # model\n",
    "from transformers import Trainer # training\n",
    "from torchvision import transforms\n",
    "from typing import Optional\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from einops import rearrange\n",
    "\n",
    "from Transformer import Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb979d2-3559-4b93-a12f-5eccae5f6d93",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a36ea6ce-25a6-4721-bf9d-4f280e0ba038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VQA Dataset\n",
    "\n",
    "class VQA_Dataset(Dataset):\n",
    "    def __init__(self, csv_path, img_root_dir, image_res, pretrained_tokenizer, is_train=True):\n",
    "        self.is_train = is_train\n",
    "        self.dataset = pd.read_csv(csv_path)\n",
    "        self.img_root_dir = img_root_dir\n",
    "        self.img_path_list = np.asarray(self.dataset['Figure_path'])\n",
    "        self.question_list = np.asarray(self.dataset['Question'])\n",
    "        self.choice_list = np.asarray(self.dataset.iloc[:,-5:-1])\n",
    "        self.answer_list = np.asarray(self.dataset['Answer'])\n",
    "        self.answer_label_list = np.asarray(self.dataset['Answer_label'])\n",
    "        \n",
    "        # initialize bert tokenizer + special tokens dict from pretrained llama model\n",
    "        self.tokenizer = LlamaTokenizer.from_pretrained(pretrained_tokenizer) # explore diff models - can fine-tune on medical but prob not too relevant\n",
    "        special_tokens_dict = {'mask_token': \"</s>\",\n",
    "                               'eos_token': \"</s>\",\n",
    "                               'bos_token': \"<s>\",\n",
    "                               'unk_token': \"<unk>\"}\n",
    "        self.tokenizer.add_special_tokens(special_tokens_dict)\n",
    "        self.tokenizer.pad_token_id=0\n",
    "    \n",
    "        # initialize, augment, and normalize torch img transformer\n",
    "        normalize = transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "        self.transform = transforms.Compose([                        \n",
    "                transforms.RandomResizedCrop([image_res,image_res],scale=(0.2, 1.0), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                #RandomAugment(2,7,isPIL=True,augs=['Identity','Equalize','Brightness','Sharpness', 'ShearX', 'ShearY', 'TranslateX', 'TranslateY', 'Rotate']),     \n",
    "                transforms.ToTensor(),\n",
    "                normalize,\n",
    "            ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_path_list)\n",
    "    \n",
    "    def encode_text(self, question_text, question_text_with_answer, mask_token= '</s>', pad_token='<unk>', eos_token = '</s>'):\n",
    "        # encode text using bert / llama tokenizer\n",
    "        def measure_text_len(text):\n",
    "            text_logits = self.tokenizer.encode(text)\n",
    "            return len(text_logits) - 1\n",
    "\n",
    "        question_text_with_answer_logits = question_text_with_answer.split()\n",
    "        question_text_logits = question_text.split()\n",
    "        bert_input_logits = []\n",
    "        output_mask = []\n",
    "        bert_label_logits = []\n",
    "\n",
    "        # create output mask by comparing len of text with text + answers\n",
    "        for i, logit in enumerate(question_text_with_answer_logits):\n",
    "            if i < len(question_text_logits):\n",
    "                word_len = measure_text_len(logit)\n",
    "                bert_input_logits += [logit]\n",
    "                bert_label_logits += [pad_token] * word_len\n",
    "                output_mask += [0] * word_len\n",
    "            else:\n",
    "                word_len = measure_text_len(logit)\n",
    "                bert_input_logits += [mask_token] * word_len\n",
    "                bert_label_logits += [logit]\n",
    "                output_mask += [1] * word_len\n",
    "\n",
    "        bert_input_logits += [eos_token]\n",
    "        bert_label_logits += [eos_token]\n",
    "        bert_input = ' '.join(bert_input_logits)\n",
    "        bert_label = ' '.join(bert_label_logits)\n",
    "        return bert_input, bert_label\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_root_dir, self.img_path_list[idx])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "\n",
    "        question = self.question_list[idx]\n",
    "        answer = self.answer_list[idx]\n",
    "        answer_label = self.answer_label_list[idx]\n",
    "        choice_A = str(self.choice_list[idx][0])\n",
    "        choice_B = str(self.choice_list[idx][1])\n",
    "        choice_C = str(self.choice_list[idx][2])\n",
    "        choice_D = str(self.choice_list[idx][3])\n",
    "        \n",
    "        # encode text logits + mlm\n",
    "        question_text = 'Question: '+ question + 'The choices are: '  + choice_A + ' ' + choice_B + ' ' + choice_C + ' ' + choice_D + 'The Answer is: '\n",
    "        question_text_with_answer = 'Question: '+ question + 'The choices are: '  + choice_A + ' ' + choice_B + ' ' + choice_C + ' ' + choice_D + 'The Answer is: ' + answer_label\n",
    "        bert_input, bert_label = self.encode_text(question_text,question_text_with_answer)\n",
    "\n",
    "        if self.is_train:\n",
    "            encoded_input = self.tokenizer(bert_input, add_special_tokens=True, padding='max_length', truncation=True, max_length= 256)\n",
    "            encoded_label = self.tokenizer(bert_label, add_special_tokens=True, padding='max_length', truncation=True, max_length= 256)\n",
    "        else:\n",
    "            encoded_input = self.tokenizer(bert_input, add_special_tokens=True, padding='max_length', truncation=True, max_length= 256,return_tensors=\"pt\")\n",
    "            encoded_label = self.tokenizer(bert_label, add_special_tokens=True, padding='max_length', truncation=True, max_length= 256,return_tensors=\"pt\")\n",
    "\n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"text_logits\": encoded_input['input_ids'],\n",
    "            \"attention_mask_logits\": encoded_input['attention_mask'],\n",
    "            \"label\": encoded_label['input_ids'],\n",
    "            \"image_path\": img_path,\n",
    "            'Choice_A': choice_A,\n",
    "            'Choice_B': choice_B,\n",
    "            'Choice_C': choice_C,\n",
    "            'Choice_D': choice_D,\n",
    "            'Answer': answer,\n",
    "            'Answer_label': answer_label,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "53a80299-eb05-466d-8bb4-c8ce99336c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model config\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    embed_dim: Optional[int] = field(default=768)\n",
    "    pretrained_tokenizer:  Optional[str] = field(default=\"TencentARC/LLaMA-Pro-8B\") # TBD\n",
    "    pretrained_model: Optional[str] = field(default=\"meta-llama/Llama-2-7b-hf\")\n",
    "    image_encoder: Optional[str] = field(default=\"CLIP\")\n",
    "    # pmcclip_pretrained: Optional[str] = field(default=\"./models/pmc_clip/checkpoint.pt\")\n",
    "    clip_pretrained: Optional[str] = field(default=\"openai/clip-vit-base-patch32\")\n",
    "    # ckp: Optional[str] = field(default=\"./Results/VQA_lora_noclip/vqa/checkpoint-6500\")\n",
    "\n",
    "# data config\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    image_res: Optional[int] = field(default=512)\n",
    "    img_root_dir: str = field(default='images/')\n",
    "    Train_path: str = field(default='data/train.csv')\n",
    "    Test_path: str= field(default='data/test.csv')\n",
    "    \n",
    "# optimization, training data, etc.\n",
    "@dataclass\n",
    "class TrainingArguments(transformers.TrainingArguments):\n",
    "    output_dir: Optional[str] = field(default=\"results\")\n",
    "    cache_dir: Optional[str] = field(default=None)\n",
    "    optim: str = field(default=\"adamw_torch\")\n",
    "    logging_dir: Optional[str] = field(default=\"logs\")\n",
    "    logging_steps: Optional[int] = field(default=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0091c0e5-958d-40e4-9680-9738f98f6416",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = transformers.HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))\n",
    "parser.add_argument(\"-f\", \"--file\", required=False)\n",
    "model_args, data_args, training_args = parser.parse_args_into_dataclasses()[:3]\n",
    "\n",
    "Train_dataset = VQA_Dataset(data_args.Train_path, data_args.img_root_dir, data_args.image_res, model_args.pretrained_tokenizer, is_train=True)\n",
    "Test_dataset = VQA_Dataset(data_args.Test_path, data_args.img_root_dir, data_args.image_res, model_args.pretrained_tokenizer, is_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ab4839ca-6d8f-4cf4-bde2-e094d55dac46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['image', 'text_logits', 'attention_mask_logits', 'label', 'image_path', 'Choice_A', 'Choice_B', 'Choice_C', 'Choice_D', 'Answer', 'Answer_label'])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# single training example\n",
    "Train_dataset[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7511071f-4bee-4d45-b690-d353acf6a546",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db39d7f-69c6-43e8-9c2d-1b51c9ef4cee",
   "metadata": {},
   "source": [
    "### Architecture\n",
    "\n",
    "1. **Text Embedding Layer (text_embedder):** This layer is responsible for embedding textual inputs. It consists of a linear transformation (fully connected layer) that maps input features (of size 4096) to an embedding space of size 768.\n",
    "\n",
    "2. **Image Encoder (image_encoder):** This component processes visual inputs. It uses the CLIPVisionModel, which incorporates a CLIPVisionTransformer. This transformer model consists of several components:\n",
    "\n",
    "3. **CLIPVisionEmbeddings:** Handles embeddings for visual inputs, including patch embeddings and position embeddings.\n",
    "\n",
    "4. **CLIPEncoder:** Contains a stack of CLIP encoder layers. Each layer includes self-attention mechanisms and feedforward neural networks (MLPs).\n",
    "\n",
    "5. **LayerNorm:** Applies layer normalization to the output of the encoder.\n",
    "\n",
    "6. **Fusion Module (fusion_module):** This module fuses information from both textual and visual inputs. It consists of several Residual Attention Blocks, each containing a multihead attention mechanism, layer normalization, and a feedforward neural network.\n",
    "\n",
    "7. **Query Decoder Layer (fquery_decoder_layer):** This layer is part of the query decoder. It includes a transformer decoder layer, which consists of self-attention mechanisms, multihead attention mechanisms, linear transformations, layer normalization, and dropout.\n",
    "\n",
    "8. **Query Decoder (fquery_decoder):** This component comprises a stack of transformer decoder layers.\n",
    "\n",
    "9. **Softmax Layer (softmax):** Applies the softmax function to the output logits along the specified dimension (-1, which is typically the last dimension) to obtain probability scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf0882b-5f78-4f58-8108-f0a2cc312773",
   "metadata": {},
   "source": [
    "### Forward Step\n",
    "1. **Encode Images (CLIP):**\n",
    "Use CLIP image encoder to encode input images, slicing off the first token.\n",
    "\n",
    "2. **Encode Text:**\n",
    "Encode input text using specified text encoder (BERT or bioBERT). Takes *text_logits* and *attention_mask_logits* as inputs.\n",
    "\n",
    "3. **Concatenate Features:**\n",
    "Concatenate encoded image and text features, adding special tokens for images.\n",
    "\n",
    "4. **Fusion with Transformer:**\n",
    "Pass concatenated features through a transformer-based fusion module.\n",
    "\n",
    "5. **Masked Language Modeling (MLM):**\n",
    "Apply MLM to fused features, generating predictions for masked tokens.\n",
    "\n",
    "6. **Softmax Activation:**\n",
    "Apply softmax activation to output logits, producing probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "19b186d2-800e-464a-a1c8-019b2f2022cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert hyperparams\n",
    "@dataclass\n",
    "class CLIPconfig:\n",
    "    bert_model: str = field(default='dmis-lab/biobert-v1.1')\n",
    "    MOMENTUM: float = 0.5  # 0.99\n",
    "    context_length: int = 256\n",
    "    vocab_size: int = 32000\n",
    "    width: int = 768\n",
    "    heads: int = 8\n",
    "    layers: int = 12\n",
    "    fusion_layers: int = 1\n",
    "    \n",
    "# peft for fine tuning + lora\n",
    "@dataclass\n",
    "class PEFTconfig:\n",
    "    use_pretrained_peft: bool = field(default=True)\n",
    "    model: str = field(default='lora')\n",
    "    lora_rank: int = field(default=8) # dimension of low-rank matrices\n",
    "    lora_alpha: int = field(default=32) # scaling factor for the low-rank matrices\n",
    "    lora_dropout: int = field(default=0.1) # dropout probability of the LoRA layers\n",
    "    # num_virtual_tokens: int = field(default=32)\n",
    "    # mapping_hidden_dim: int = field(default=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "299561f2-7b0e-42bf-a522-55c12d8a3ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_peft_config(peft_args: PEFTconfig):\n",
    "    if peft_args.use_pretrained_peft:\n",
    "        peft_config = LoraConfig(\n",
    "            task_type = TaskType.CAUSAL_LM, inference_mode=False,\n",
    "            r = peft_args.lora_rank,\n",
    "            lora_alpha = peft_args.lora_alpha, \n",
    "            lora_dropout = peft_args.lora_dropout\n",
    "        )\n",
    "    else:\n",
    "        # TODO: lora fine tuning on text encoder for medical domain\n",
    "        # perf_config = ...\n",
    "        pass\n",
    "    return peft_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "447484e4-e3a1-4bbb-8220-b9aa8ff080c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = transformers.HfArgumentParser((CLIPconfig, PEFTconfig))\n",
    "parser.add_argument(\"-f\", \"--file\", required=False)\n",
    "clip_args, peft_args = parser.parse_args_into_dataclasses()[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "aa8258e6-bfe8-4cc0-9cbd-4d9eae13dd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQA_Model(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(VQA_Model, self).__init__()\n",
    "        embed_dim = config.embed_dim\n",
    "        self.tokenizer = LlamaTokenizer.from_pretrained(config.pretrained_tokenizer)\n",
    "        # self.llama_model = transformers.LlamaModel.from_pretrained(config.pretrained_model, token = \"\")\n",
    "        # peft_config = get_peft_config(PEFTconfig)\n",
    "        self.text_encoder = AutoTokenizer.from_pretrained(config.pretrained_tokenizer) # currently just biobert/tencent; TODO: change to peft model\n",
    "        self.text_embedder = nn.Sequential(nn.Linear(4096, embed_dim))\n",
    "        self.context_length = 256\n",
    "        \n",
    "        # clip vision encoder\n",
    "        if config.image_encoder == \"CLIP\":\n",
    "            self.image_encoder_name = \"CLIP\"\n",
    "            clip_config = CLIPVisionConfig(image_size=512)\n",
    "            self.image_encoder = CLIPVisionModel(clip_config).from_pretrained(config.clip_pretrained) # \"openai/clip-vit-base-patch32\"\n",
    "            self.image_encoder.vision_model.embeddings = transformers.models.clip.modeling_clip.CLIPVisionEmbeddings(clip_config)\n",
    "        else:\n",
    "            self.image_encoder_name = \"PMC_CLIP\"\n",
    "            # TODO: pmc clip handling...\n",
    "        # other hidden layers\n",
    "        self.fquery_input = nn.Parameter(torch.empty(32, 768))\n",
    "        self.fquery_decoder_layer = nn.TransformerDecoderLayer(embed_dim, nhead=4, dim_feedforward=768, dropout=0.1, activation='relu', norm_first=True)\n",
    "        self.fquery_decoder_norm = nn.LayerNorm(embed_dim)\n",
    "        self.fquery_decoder = nn.TransformerDecoder(self.fquery_decoder_layer, 12, self.fquery_decoder_norm)\n",
    "\n",
    "        text_cfg = CLIPconfig\n",
    "        self.transformer_width = text_cfg.width\n",
    "        self.positional_embedding = nn.Parameter(torch.empty(CLIPconfig.context_length, text_cfg.width))\n",
    "        self.text_projection = nn.Parameter(torch.empty(text_cfg.width, embed_dim))\n",
    "        self.mlm_projection = nn.Parameter(torch.empty(text_cfg.width, text_cfg.vocab_size))\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "        \n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "        self.img_special_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.fusion_module = Transformer(\n",
    "            width=text_cfg.width,\n",
    "            layers=text_cfg.fusion_layers,\n",
    "            heads=text_cfg.heads,\n",
    "        )\n",
    "        self.register_buffer('attn_mask', self.build_attention_mask(), persistent=False)\n",
    "        self.init_parameters()\n",
    "\n",
    "    def build_attention_mask(self):\n",
    "        # lazily create causal attention mask, with full attention between the vision tokens\n",
    "        # pytorch uses additive attention mask; fill with -inf\n",
    "        mask = torch.empty(CLIPconfig.context_length, CLIPconfig.context_length)\n",
    "        mask.fill_(float(\"-inf\"))\n",
    "        mask.triu_(1)  # zero out the lower diagonal\n",
    "        return mask\n",
    "\n",
    "    def init_parameters(self):\n",
    "        nn.init.normal_(self.positional_embedding, std=0.01)\n",
    "        nn.init.constant_(self.logit_scale, np.log(1/0.07))\n",
    "        if self.text_projection is not None:\n",
    "            nn.init.normal_(self.text_projection, std=self.transformer_width ** -0.5)\n",
    "        if self.mlm_projection is not None:\n",
    "            nn.init.normal_(self.mlm_projection, std=self.transformer_width ** -0.5)\n",
    "    \n",
    "    def forward(self, image, text_logits, attention_mask_logits):\n",
    "        batch_size, _ = text_logits.shape\n",
    "        if self.image_encoder_name == 'CLIP':\n",
    "            # resize image to batch_size, \n",
    "            image_logits = self.image_encoder(image).last_hidden_state[:,1:,:]\n",
    "        else:\n",
    "            print('not supported') # TODO\n",
    "\n",
    "        image_query_features = self.fquery_input.unsqueeze(0).expand(batch_size, -1, -1) # resize to (b . .)\n",
    "        image_logits = self.fquery_decoder(image_query_features.transpose(0,1), image_logits.transpose(0,1)).transpose(0,1)\n",
    "        \n",
    "        question_features  = self.text_encoder(text = text_logits, attention_mask = attention_mask_logits)[0] # TODO: TRAIN TEXT ENCODER\n",
    "        question_features = rearrange(question_features, 'b n d -> (b n) d') # (embed dim, 4096)\n",
    "        question_features = self.text_embedder(question_features)\n",
    "        x = rearrange(question_features,'(b n) d -> b n d', b = batchsize)\n",
    "        \n",
    "        B, _len, _dim = x.shape\n",
    "        img_special_tokens = self.img_special_token.expand(B, -1, -1) # (128, 1, embed_dim)\n",
    "        x = torch.cat([x, img_special_tokens, image_logits], dim=1) # combine image and text features\n",
    "        x = x.permute(1, 0, 2)  # batch-first -> sequence-first\n",
    "        x = self.fusion_module(x)\n",
    "        x = x.permute(1, 0, 2)  # sequence-first -> batch-first\n",
    "        x = x[:, :-33, :]  # Remove token (img_special_token, img)\n",
    "        out = self.softmax(x @ self.mlm_projection)  # [batch_size=128, n_ctx=77, vocab_size=49409]\n",
    "        return out "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ac6881-c21d-44ed-9d9b-cbe662b5b76d",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f13996b4-2656-43fb-9d51-590620519169",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQATrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        image = inputs['image']  \n",
    "        label = inputs['labels'].to(dtype=torch.long) \n",
    "        input_logits = inputs['text_logits'] \n",
    "        attention_mask_logits = inputs['attention_mask_logits'] \n",
    "        outputs = model(image, input_logits, attention_mask_logits)\n",
    "        loss = F.nll_loss(outputs.transpose(1, 2), label, ignore_index=0)\n",
    "        return (loss, {'outputs':outputs}) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1277ae03-037c-4396-a9d6-74f8cf60fbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VQA_Model(model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c5f5d568-c8ee-47aa-9945-018e5bef8a90",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m VQATrainer(model\u001b[38;5;241m=\u001b[39mmodel, \n\u001b[1;32m      2\u001b[0m                      train_dataset \u001b[38;5;241m=\u001b[39m Train_dataset, \n\u001b[1;32m      3\u001b[0m                      eval_dataset \u001b[38;5;241m=\u001b[39m Test_dataset,\n\u001b[1;32m      4\u001b[0m                      args\u001b[38;5;241m=\u001b[39mtraining_args\n\u001b[1;32m      5\u001b[0m                     )\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_state()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/trainer.py:1860\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1857\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1860\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1863\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1864\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1865\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1866\u001b[0m ):\n\u001b[1;32m   1867\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/trainer.py:2725\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2722\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2724\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2725\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2727\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2728\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[60], line 7\u001b[0m, in \u001b[0;36mVQATrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m      5\u001b[0m input_logits \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_logits\u001b[39m\u001b[38;5;124m'\u001b[39m] \n\u001b[1;32m      6\u001b[0m attention_mask_logits \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask_logits\u001b[39m\u001b[38;5;124m'\u001b[39m] \n\u001b[0;32m----> 7\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask_logits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(outputs\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m), label, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (loss, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m'\u001b[39m:outputs}) \u001b[38;5;28;01mif\u001b[39;00m return_outputs \u001b[38;5;28;01melse\u001b[39;00m loss\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[59], line 71\u001b[0m, in \u001b[0;36mVQA_Model.forward\u001b[0;34m(self, image, text_logits, attention_mask_logits)\u001b[0m\n\u001b[1;32m     68\u001b[0m image_query_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfquery_input\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# resize to (b . .)\u001b[39;00m\n\u001b[1;32m     69\u001b[0m image_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfquery_decoder(image_query_features\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m), image_logits\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 71\u001b[0m question_features  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtext_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattention_mask_logits\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     72\u001b[0m question_features \u001b[38;5;241m=\u001b[39m rearrange(question_features, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb n d -> (b n) d\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# (embed dim, 4096)\u001b[39;00m\n\u001b[1;32m     73\u001b[0m question_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_embedder(question_features)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2798\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2796\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2797\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2798\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2799\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2800\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2856\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2853\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2855\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text):\n\u001b[0;32m-> 2856\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2857\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2858\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2859\u001b[0m     )\n\u001b[1;32m   2861\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text_pair):\n\u001b[1;32m   2862\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2863\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2865\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "trainer = VQATrainer(model=model, \n",
    "                     train_dataset = Train_dataset, \n",
    "                     eval_dataset = Test_dataset,\n",
    "                     args=training_args\n",
    "                    )\n",
    "trainer.train()\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2762e070-2b1d-479f-9049-09fd7eb4c9c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
